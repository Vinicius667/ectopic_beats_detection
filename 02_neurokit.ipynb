{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39720,"status":"ok","timestamp":1701127818008,"user":{"displayName":"VinÃ­cius Almeida","userId":"13624117960268289849"},"user_tz":180},"id":"szNbhQ4E7aGA","outputId":"db7b37b0-366a-491b-f7d6-8231b968d3c4"},"outputs":[],"source":["import sys\n","# Delete all global variables when re-running the notebook.\n","this = sys.modules[__name__] # type: ignore\n","for n in dir():\n","    if n in ['this', 'was_mounted']: continue\n","    if n[0]!='_': delattr(this, n)\n","\n","\n","try:\n","    was_mounted = was_mounted\n","except:\n","    was_mounted = False\n","\n","\n","import os\n","if  os.getenv(\"COLAB_RELEASE_TAG\"):\n","  is_running_on_colab = True\n","\n","else:\n","  is_running_on_colab = False\n","\n","if is_running_on_colab:\n","  packages_to_install = ['pandas==2.1.3','neurokit2', 'wfdb']\n","\n","  for package in packages_to_install:\n","    os.system(f'pip install {package}')\n","  from google.colab import drive, files\n","  code_directory = './gdrive/MyDrive/TCC/ectopic_beats_detection'\n","  if not was_mounted:\n","      drive.mount('/content/gdrive')\n","  was_mounted = True\n","  if not os.path.samefile(os.getcwd(),code_directory):\n","    os.chdir(code_directory)\n","\n","from utils import create_compare_df, create_dict_results, plot_results, calculate_metrics, resolve_relative_path\n","from globals import *\n","import sys\n","import neurokit2 as nk\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import plotly.graph_objects as go\n","import numpy as np\n","import pyarrow as pa\n","from os.path import join\n","import pyarrow as pa\n","from typing import Any, Dict, Iterable, List, Tuple, Union\n","from numpy import typing as npt\n","from utils import Processor, Processors, load_df_multi_analysis, load_record, apply_processors, correct_peaks\n","from datetime import datetime\n","import pickle\n","from multiprocessing import Pool, cpu_count\n","from timeit import default_timer as timer\n","import glob\n","\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', 40)\n","\n","df_record_lead_ann = pd.read_parquet(join(dataframes_directory, 'df_record_lead_ann.parquet'))\n","df_lead_ann_summery =  pd.read_parquet(join(dataframes_directory, 'df_lead_ann_summery.parquet'))\n","df_ann_summery = pd.read_parquet(join(dataframes_directory, 'df_ann_summery.parquet'))\n","df_code_description = pd.read_parquet(join(dataframes_directory, 'df_code_description.parquet'))\n","\n","df_multi_analysis = load_df_multi_analysis(glob.glob(join(dataframes_directory, 'dict_multi_analysis*.pickle')))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_multi_analysis[df_multi_analysis[['record_num','processor','method']].duplicated()]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_multi_analysis.groupby(['record_num']).record_num.count().sort_values().head(5)"]},{"cell_type":"markdown","metadata":{"id":"OgKgTe3dpg4N"},"source":["# Multiple record analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJhJzM_upg4P","outputId":"8bb6ec45-d102-438b-9cf6-92631d138353"},"outputs":[],"source":["was_interrupted = False\n","total_time = 300 # seconds\n","\n","offset = 100 # seconds\n","discard_start_sec = discard_end_sec = 2\n","\n","methods = ['neurokit', 'pantompkins1985', 'hamilton2002', 'martinez2004', 'christov2004',\n","               'gamboa2008', 'elgendi2010', 'engzeemod2012', 'kalidas2017', 'rodrigues2020']\n","offset = 500 # seconds\n","\n","derised_anns = LIST_BEATS_1\n","list_processors = [\n","    Processor(None),\n","    Processor('detrend', method = 'polynomial', order = 0),\n","    Processor('detrend', method = 'polynomial', order = 1),\n","]\n","\n","dict_multi_analysis = {}\n","print(f'Total time = {total_time} seconds')\n","\n","time_str = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n","\n","count_processed = 0\n","try:\n","    for idx, row in df_record_lead_ann.iterrows():\n","        if row['upper_signal'] == 'MLII':\n","            signal_track = 0\n","        elif row['lower_signal'] == 'MLII':\n","            signal_track = 1\n","        else:\n","            continue\n","\n","        record_num = row['record']\n","\n","        # Start measuring time\n","        start_time = timer()\n","\n","        print(f'Processing record {record_num}')\n","\n","        # Load record\n","        record, ann = load_record(record_num)\n","        fs = int(record.fs)  # type: ignore\n","\n","        samples = int(total_time * fs)\n","        start_samples = int(offset * fs)\n","        end_samples = start_samples + samples\n","        \n","        dict_multi_analysis[record_num] = {}\n","        for processor in list_processors:\n","            processor_name = processor.processor_name\n","            print(f'Processor =  {processor_name}')\n","            methods_done = df_multi_analysis[\n","            (df_multi_analysis.record_num == record_num)\n","            & (df_multi_analysis.signal_track == signal_track)\n","            & (df_multi_analysis.start_samples == start_samples)\n","            & (df_multi_analysis.end_samples == end_samples)\n","            & (df_multi_analysis.processor == processor_name)\n","            ].method.to_list()\n","\n","            methods_missing = [method for method in methods if method not in methods_done]\n","\n","            if len(methods_missing) == 0:\n","                print('All methods already processed')\n","                continue\n","\n","            \n","            #dict_multi_analysis[record_num][processor_name] = {}\n","\n","            #df_beats, ecg, start_samples, end_samples, fs = create_df_beats(record_num, total_time, offset, LIST_BEATS_1, signal_track)\n","\n","            # ECG signal\n","            ecg = record.p_signal[:, signal_track][start_samples:end_samples]  # type: ignore\n","\n","            if processor.processor_name:\n","                ecg = apply_processors(ecg, processor)\n","\n","            ecg = pd.Series(ecg, dtype=ECG_TYPE)\n","            ecg.index += start_samples\n","            ann_beat_indexes = pd.Series(ann.sample, dtype=INDEX_TYPE)\n","            ann_beat_symbols = pd.Series(ann.symbol, dtype=ANN_TYPE)\n","\n","            # Mask for time window and derised annotations\n","            mask_derised_ann = ann_beat_symbols.isin(derised_anns)\n","\n","            # We are only interested in samples in the time window\n","            mask_time_window = (ann_beat_indexes >= start_samples) & (\n","                ann_beat_indexes < end_samples)\n","            mask_used_ann = mask_time_window & mask_derised_ann\n","\n","            # Apply mask\n","            ann_beat_indexes = ann_beat_indexes[mask_used_ann].reset_index(drop=True)\n","            ann_beat_symbols = ann_beat_symbols[mask_used_ann].reset_index(drop=True)\n","\n","            df_beats = correct_peaks(ecg, ann_beat_indexes, fs)\n","\n","            df_beats = df_beats.rename(columns={'index': 'peak_index', 'local_max': 'cor_peak_index'}).merge(\n","                pd.DataFrame({'peak_index': ann_beat_indexes, 'symbol': ann_beat_symbols}), on='peak_index', how='left', validate='one_to_one')\n","            \n","            #df_beats.cor_peak_index = df_beats.peak_index\n","\n","            # If the peak is not corrected, use the original peak index\n","            df_beats.loc[df_beats.cor_peak_index.isna(\n","            ), 'cor_peak_index'] = df_beats.peak_index\n","\n","\n","            first_used_sample = start_samples + discard_start_sec * fs\n","            last_used_sample = end_samples - discard_end_sec * fs\n","\n","            #dict_results = create_dict_results(ecg, methods,start_samples, first_used_sample, last_used_sample, fs, discard_start_sec, discard_end_sec)\n","            p = Pool(cpu_count())\n","            list_results = p.starmap(create_dict_results, [(ecg, [method], start_samples, first_used_sample, last_used_sample, fs, discard_start_sec, discard_end_sec) for method in methods_missing])\n","            dict_results = {}\n","            for result in list_results:\n","                for key, value in result.items():\n","                    dict_results[key] = value\n","\n","            # Now the operations are performed on the time window, we can discard the first and last n seconds\n","            ecg = ecg.loc[first_used_sample:last_used_sample ]\n","            df_beats = df_beats[(df_beats.peak_index >= first_used_sample) & (df_beats.peak_index <= last_used_sample)].reset_index(drop = True)\n","\n","            df_comp_methods = create_compare_df(df_beats, dict_results)\n","\n","            dict_metrics = calculate_metrics(df_comp_methods, methods_missing)\n","\n","            for method in dict_metrics.keys():\n","                dict_metrics[method]['signal_track'] = signal_track\n","                dict_metrics[method]['start_samples'] = start_samples\n","                dict_metrics[method]['end_samples'] = end_samples\n","            \n","\n","            dict_multi_analysis[record_num][processor_name] = dict_metrics\n","            count_processed += 1\n","        end_time = timer()\n","        print(f'Time elapsed = {end_time - start_time} seconds')\n","except KeyboardInterrupt:\n","    if count_processed > 0:\n","        with open(join(dataframes_directory, f'dict_multi_analysis_interrupted_{time_str}.pickle'), 'wb') as handle:\n","            pickle.dump(dict_multi_analysis, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","        was_interrupted = True\n","\n","# save results as pickle\n","if not was_interrupted and count_processed > 0:\n","    with open(join(dataframes_directory, f'dict_multi_analysis_{time_str}.pickle'), 'wb') as handle:\n","            pickle.dump(dict_multi_analysis, handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x_xis_factor = 1\n","\n","fig = plot_results(\n","    dict_results, df_beats, ecg,\n","    [\n","        (\n","            df_code_description.symbol[~df_code_description.symbol.isin(\n","                LIST_BEATS_1)],\n","            dict(mode=\"markers\", marker=dict(size=4, color=\"black\"))\n","        ),\n","\n","        (LIST_BEATS_1, dict(mode=\"markers\", marker=dict(size=9, color=\"red\")))\n","    ],\n","\n","    x_xis_factor=1\n",")\n","\n","#Define x zoom\n","fig.update_xaxes(range=[271700,272300])\n","\n","fig.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_multi_analysis.groupby(['method'])[['accuracy']].agg(['mean', 'min', 'std']).sort_values(('accuracy', 'min'), ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_multi_analysis[df_multi_analysis.processor == 'None'].groupby(['method'])[['accuracy']].agg(['mean', 'min','std']).sort_values(('accuracy', 'min'), ascending=False) / df_multi_analysis.groupby(['method'])[['accuracy']].agg(['mean', 'min', 'std']).sort_values(('accuracy', 'min'), ascending=False) -1"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"ML","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"vscode":{"interpreter":{"hash":"4f021b72c1f7cd3898f2a25ac030750b22d22f61570d94b028ffadc231687c12"}}},"nbformat":4,"nbformat_minor":0}
