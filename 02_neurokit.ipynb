{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Delete all global variables when re-running the notebook.\n",
    "this = sys.modules[__name__]\n",
    "for n in dir():\n",
    "    if n == 'this': continue\n",
    "    if n[0]!='_': delattr(this, n)\n",
    "\n",
    "from utils import *\n",
    "from globals import *\n",
    "import sys\n",
    "import neurokit2 as nk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "from os.path import join\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_record_lead_ann = pd.read_parquet(join(dataframes_directory, 'df_record_lead_ann.parquet'))\n",
    "df_lead_ann_summery =  pd.read_parquet(join(dataframes_directory, 'df_lead_ann_summery.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only feature extracted from the ECG is the R wave. Initially, a point in the QRS complex is detected (QRS point), using the algorithm proposed by Hamilton and Tompkins [25,26]. Then, the main wave of the QRS complex (R wave) is identified in the window [QRS  280 ms, QRS Ã¾ 120 ms] by locating the point where the signal has its maximum absolute value. The RR-interval signal is constructed by measuring the time interval between successive R waves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting black\n",
      "  Downloading black-23.11.0-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\vinicius\\anaconda3\\envs\\ml\\lib\\site-packages (from black) (8.0.1)\n",
      "Requirement already satisfied: platformdirs>=2 in c:\\users\\vinicius\\anaconda3\\envs\\ml\\lib\\site-packages (from black) (3.2.0)\n",
      "Collecting pathspec>=0.9.0\n",
      "  Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in c:\\users\\vinicius\\anaconda3\\envs\\ml\\lib\\site-packages (from black) (4.5.0)\n",
      "Collecting mypy-extensions>=0.4.3\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting tomli>=1.1.0\n",
      "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting packaging>=22.0\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 53.0/53.0 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\vinicius\\anaconda3\\envs\\ml\\lib\\site-packages (from click>=8.0.0->black) (0.4.6)\n",
      "Installing collected packages: tomli, pathspec, packaging, mypy-extensions, black\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "Successfully installed black-23.11.0 mypy-extensions-1.0.0 packaging-23.2 pathspec-0.11.2 tomli-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-appengine-logging 1.1.0 requires google-api-core[grpc]<3.0.0dev,>=1.22.2, which is not installed.\n",
      "drf-yasg 1.20.0 requires Django>=2.2.16, which is not installed.\n",
      "mapillary 1.0.11 requires bleach==3.3.0, but you have bleach 5.0.1 which is incompatible.\n",
      "mapillary 1.0.11 requires certifi==2021.5.30, but you have certifi 2022.12.7 which is incompatible.\n",
      "mapillary 1.0.11 requires packaging==21.3, but you have packaging 23.2 which is incompatible.\n",
      "mapillary 1.0.11 requires protobuf==3.17.3, but you have protobuf 3.20.3 which is incompatible.\n",
      "mapillary 1.0.11 requires tqdm==4.61.1, but you have tqdm 4.65.0 which is incompatible.\n",
      "mapillary 1.0.11 requires urllib3==1.26.5, but you have urllib3 1.26.16 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install black"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single record analysis\n",
    "We analyze just one record from the dataset. Later we will analyze all the records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load record\n",
    "\n",
    "record_num = 212\n",
    "record, ann =  load_record(record_num)\n",
    "fs = record.fs\n",
    "\n",
    "total_time = 34 # seconds\n",
    "offset = 80 # seconds\n",
    "samples = int(total_time * fs)\n",
    "start_samples = int(offset * fs)\n",
    "end_samples = start_samples + samples\n",
    "\n",
    "# Discard first n seconds\n",
    "discard_samples = 2 * fs # 2 seconds\n",
    "first_used_sample = start_samples + discard_samples\n",
    "last_used_sample = end_samples - discard_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECG signal\n",
    "ecg = pd.Series(record.p_signal[:,0], dtype=ECG_TYPE)[start_samples:end_samples]\n",
    "ann_beat_indexes = pd.Series(ann.sample, dtype=INDEX_TYPE)\n",
    "ann_beat_symbols= pd.Series(ann.symbol, dtype=ANN_TYPE)\n",
    "\n",
    "# Derised annotations (N - normal beat)\n",
    "derised_anns = ['N', 'P', 'f', 'L', 'R', 'Q']\n",
    "\n",
    "# Mask for time window and derised annotations\n",
    "mask_derised_ann = ann_beat_symbols.isin(derised_anns)\n",
    "\n",
    "# We are only interested in samples in the time window\n",
    "mask_time_window = (ann_beat_indexes >= start_samples) & (ann_beat_indexes < end_samples)\n",
    "\n",
    "mask_used_ann = mask_time_window & mask_derised_ann\n",
    "\n",
    "# Apply mask\n",
    "ann_beat_indexes = ann_beat_indexes[mask_used_ann].reset_index(drop = True)\n",
    "ann_beat_symbols = ann_beat_symbols[mask_used_ann].reset_index(drop = True)\n",
    "\n",
    "df_beats  = correct_peaks(ecg, ann_beat_indexes, fs)\n",
    "\n",
    "\n",
    "df_beats = df_beats.rename(columns = {'index' : 'peak_index', 'local_max' : 'cor_peak_index'}).merge(\n",
    "    pd.DataFrame({'peak_index' : ann_beat_indexes, 'symbol' : ann_beat_symbols}), on = 'peak_index', how = 'left', validate = 'one_to_one')\n",
    "\n",
    "# If the peak is not corrected, use the original peak index\n",
    "df_beats.loc[df_beats.cor_peak_index.isna(), 'cor_peak_index'] = df_beats.peak_index\n",
    "\n",
    "df_beats.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods=  ['hamilton2002', 'kalidas2017', 'rodrigues2020']\n",
    "\n",
    "dict_results = {}\n",
    "\n",
    "for method in methods:\n",
    "    method_beat_indexes = find_peaks(ecg, fs, method)\n",
    "    # Fix index\n",
    "    method_beat_indexes += start_samples\n",
    "    df_method_beats = correct_peaks(ecg, method_beat_indexes, fs)\n",
    "\n",
    "\n",
    "    # When the method fails to detect a peak, the index is set to NaN. We replace it with the original index.\n",
    "    df_method_beats.loc[df_method_beats.local_max.isna(), 'local_max'] = df_method_beats.local_max\n",
    "\n",
    "    local_max = df_method_beats.local_max\n",
    "    df_method_beats = df_method_beats[(local_max >= first_used_sample) & (local_max < last_used_sample)]\n",
    "    # Store results in dict\n",
    "    dict_results[method] = df_method_beats\n",
    "    #dict_results[method] = pd.Series(result)\n",
    "\n",
    "\n",
    "# Now the operations are performed on the time window, we can discard the first and last n seconds\n",
    "\n",
    "ecg = ecg.loc[first_used_sample:last_used_sample]\n",
    "df_beats = df_beats[(df_beats.peak_index >= first_used_sample) & (df_beats.peak_index <= last_used_sample)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ECG signal\n",
    "\n",
    "x_xis_factor = 1 #1/fs # 1/fs = seconds, 1 = samples\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=ecg.index*x_xis_factor, y=ecg, name=\"ECG\"))\n",
    "\n",
    "\n",
    "for desired_ann in derised_anns:\n",
    "    # Get the samples of the desired annotations\n",
    "    df_beats_desired = df_beats[df_beats.symbol == desired_ann]\n",
    "    desired_peak_indexes = df_beats_desired.cor_peak_index\n",
    "    \n",
    "    # Plot the annotations\n",
    "    fig.add_trace(go.Scatter(x=desired_peak_indexes*x_xis_factor, y=ecg.loc[desired_peak_indexes], mode=\"markers\", name=desired_ann, marker=dict(size=7, color=\"red\")))\n",
    "\n",
    "\n",
    "# Plot the R peaks from the methods\n",
    "for method in methods:\n",
    "    df_method_beats = dict_results[method]\n",
    "    peak_indexes = df_method_beats.local_max\n",
    "    fig.add_trace(go.Scatter(x=peak_indexes*x_xis_factor, y=ecg.loc[peak_indexes], mode=\"markers\", name=method, marker=dict(size=7)))\n",
    "\n",
    "#Define x zoom\n",
    "fig.update_xaxes(range=[first_used_sample * x_xis_factor, (first_used_sample + 10 * fs) * x_xis_factor ])\n",
    "\n",
    "# Remove borders\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=15, b=0),\n",
    "    paper_bgcolor=\"white\",\n",
    ")\n",
    "\n",
    "# Add range slider\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        rangeslider=dict(\n",
    "            visible=True\n",
    "        ),\n",
    "        type=\"linear\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in methods:\n",
    "    true_positive = ((df_comp_methods['ann'] == True) & (df_comp_methods[method]) == True).values.sum()\n",
    "    true_negative = ((df_comp_methods['ann'] == False) & (df_comp_methods[method]) == False).values.sum()\n",
    "    false_positive = ((df_comp_methods['ann'] == False) & (df_comp_methods[method] == True)).values.sum()\n",
    "    false_negative = ((df_comp_methods['ann'] == True) & (df_comp_methods[method] == False)).values.sum()\n",
    "\n",
    "\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n",
    "\n",
    "\n",
    "    print(f'{method}:\\n\\tPrecision: {100*precision:.2f} %\\n\\tRecall: {100*recall:.2f} %\\n\\tAccuracy: {100*accuracy:.2f} %\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 30 # seconds\n",
    "offset = 17 # seconds\n",
    "samples = int(total_time * fs)\n",
    "start_samples = int(offset * fs)\n",
    "end_samples = start_samples + samples\n",
    "\n",
    "# Discard first n seconds\n",
    "discard_samples = 2 * fs # 2 seconds\n",
    "first_used_sample = start_samples + discard_samples\n",
    "\n",
    "for idx, row in df_record_lead_ann.iterrows():\n",
    "    if row['upper_signal'] == 'MLII':\n",
    "        signal_track = 0\n",
    "    elif row['lower_signal'] == 'MLII':\n",
    "        signal_track = 1\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    record_num = row['record']\n",
    "    record, ann =  load_record(record_num)\n",
    "    fs = record.fs\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f021b72c1f7cd3898f2a25ac030750b22d22f61570d94b028ffadc231687c12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
