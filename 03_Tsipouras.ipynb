{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Delete all global variables when re-running the notebook.\n",
    "this = sys.modules[__name__] # type: ignore\n",
    "for n in dir():\n",
    "    if n in ['this', 'was_mounted']: continue\n",
    "    if n[0]!='_': delattr(this, n)\n",
    "\n",
    "\n",
    "try:\n",
    "    was_mounted = was_mounted\n",
    "except:\n",
    "    was_mounted = False\n",
    "\n",
    "\n",
    "import os\n",
    "if  os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "  is_running_on_colab = True\n",
    "\n",
    "else:\n",
    "  is_running_on_colab = False\n",
    "\n",
    "if is_running_on_colab:\n",
    "  packages_to_install = ['pandas==2.1.3','neurokit2', 'wfdb']\n",
    "\n",
    "  for package in packages_to_install:\n",
    "    os.system(f'pip install {package}')\n",
    "  from google.colab import drive, files\n",
    "  code_directory = './gdrive/MyDrive/TCC/ectopic_beats_detection'\n",
    "  if not was_mounted:\n",
    "      drive.mount('/content/gdrive')\n",
    "  was_mounted = True\n",
    "  if not os.path.samefile(os.getcwd(),code_directory):\n",
    "    os.chdir(code_directory)\n",
    "\n",
    "from utils import create_compare_df, create_dict_results, plot_results, calculate_metrics, resolve_relative_path\n",
    "from globals import *\n",
    "import sys\n",
    "import neurokit2 as nk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "from os.path import join\n",
    "import pyarrow as pa\n",
    "from typing import Any, Dict, Iterable, List, Tuple, Union\n",
    "from numpy import typing as npt\n",
    "from utils import Processor, Processors, load_df_multi_analysis, load_record, apply_processors, correct_peaks\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from timeit import default_timer as timer\n",
    "import glob\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 40)\n",
    "\n",
    "df_record_lead_ann = pd.read_parquet(join(dataframes_directory, 'df_record_lead_ann.parquet'))\n",
    "df_lead_ann_summery =  pd.read_parquet(join(dataframes_directory, 'df_lead_ann_summery.parquet'))\n",
    "df_ann_summery = pd.read_parquet(join(dataframes_directory, 'df_ann_summery.parquet'))\n",
    "df_code_description = pd.read_parquet(join(dataframes_directory, 'df_code_description.parquet'))\n",
    "\n",
    "df_multi_analysis = load_df_multi_analysis(glob.glob(join(dataframes_directory, 'dict_multi_analysis*.pickle')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_num</th>\n",
       "      <th>processor</th>\n",
       "      <th>method</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>signal_track</th>\n",
       "      <th>start_samples</th>\n",
       "      <th>end_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [record_num, processor, method, accuracy, precision, signal_track, start_samples, end_samples]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_multi_analysis[df_multi_analysis[['record_num','processor','method']].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "record_num\n",
       "207    27\n",
       "100    30\n",
       "205    30\n",
       "208    30\n",
       "209    30\n",
       "Name: record_num, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_multi_analysis.groupby(['record_num']).record_num.count().sort_values().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing record 228\n"
     ]
    }
   ],
   "source": [
    "was_interrupted = False\n",
    "total_time = 300 # seconds\n",
    "\n",
    "offset = 100 # seconds\n",
    "discard_start_sec = discard_end_sec = 2\n",
    "\n",
    "methods = ['neurokit', 'pantompkins1985', 'hamilton2002', 'martinez2004', 'christov2004',\n",
    "               'gamboa2008', 'elgendi2010', 'engzeemod2012', 'kalidas2017', 'rodrigues2020']\n",
    "offset = 500 # seconds\n",
    "\n",
    "derised_anns = LIST_BEATS_1\n",
    "\n",
    "try:\n",
    "    for idx, row in df_record_lead_ann.iterrows():\n",
    "        if row['upper_signal'] == 'MLII':\n",
    "            signal_track = 0\n",
    "        elif row['lower_signal'] == 'MLII':\n",
    "            signal_track = 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        record_num = row['record'] + 128\n",
    "\n",
    "        # Start measuring time\n",
    "        start_time = timer()\n",
    "\n",
    "        print(f'Processing record {record_num}')\n",
    "\n",
    "        # Load record\n",
    "        record, ann = load_record(record_num)\n",
    "        fs = int(record.fs)  # type: ignore\n",
    "\n",
    "        samples = int(total_time * fs)\n",
    "        start_samples = int(offset * fs)\n",
    "        end_samples = start_samples + samples\n",
    "        first_used_sample = start_samples + discard_start_sec * fs\n",
    "        last_used_sample = end_samples - discard_end_sec * fs\n",
    "\n",
    "        ecg = record.p_signal[:, signal_track][start_samples:end_samples]  # type: ignore\n",
    "        ecg = pd.Series(ecg, dtype=ECG_TYPE)\n",
    "        ecg.index += start_samples\n",
    "\n",
    "        ann_beat_indexes = pd.Series(ann.sample, dtype=INDEX_TYPE)\n",
    "        ann_beat_symbols = pd.Series(ann.symbol, dtype=ANN_TYPE)\n",
    "\n",
    "        # Mask for time window and derised annotations\n",
    "        mask_derised_ann = ann_beat_symbols.isin(derised_anns)\n",
    "\n",
    "        # We are only interested in samples in the time window\n",
    "        mask_time_window = (ann_beat_indexes >= start_samples) & (\n",
    "            ann_beat_indexes < end_samples)\n",
    "        mask_used_ann = mask_time_window & mask_derised_ann\n",
    "\n",
    "        # Apply mask\n",
    "        ann_beat_indexes = ann_beat_indexes[mask_used_ann].reset_index(drop=True)\n",
    "        ann_beat_symbols = ann_beat_symbols[mask_used_ann].reset_index(drop=True)\n",
    "\n",
    "        df_beats = correct_peaks(ecg, ann_beat_indexes, fs)\n",
    "\n",
    "        df_beats = df_beats.rename(columns={'index': 'peak_index', 'local_max': 'cor_peak_index'}).merge(\n",
    "            pd.DataFrame({'peak_index': ann_beat_indexes, 'symbol': ann_beat_symbols}), on='peak_index', how='left', validate='one_to_one')\n",
    "\n",
    "        #df_beats.cor_peak_index = df_beats.peak_index\n",
    "\n",
    "        # If the peak is not corrected, use the original peak index\n",
    "        df_beats.loc[df_beats.cor_peak_index.isna(\n",
    "        ), 'cor_peak_index'] = df_beats.peak_index\n",
    "\n",
    "        raise\n",
    "except RuntimeError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'hamilton2002'\n",
    "\n",
    "peaks = dict_results = create_dict_results(ecg, method,start_samples, first_used_sample, last_used_sample, fs, discard_start_sec, discard_end_sec)[method]['local_max'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat numpy warnings as errors\n",
    "np.seterr(all='raise')\n",
    "\n",
    "# By default, classify all beats as normal\n",
    "classifcations = np.ones(peaks.shape[0], dtype=np.int8)\n",
    "\n",
    "# Ventricular fibrillation (VF)\n",
    "vf_episode = False\n",
    "vf_quant = 0\n",
    "\n",
    "\n",
    "rr = np.diff(peaks) / fs\n",
    "\n",
    "for i in range(1,peaks.shape[0] - 3):\n",
    "    rr1_idx = i; rr1 = rr[rr1_idx]\n",
    "    rr2_idx = i + 1; rr2 = rr[rr2_idx]\n",
    "    rr3_idx = i + 2; rr3 = rr[rr3_idx]\n",
    "\n",
    "    if not vf_episode:\n",
    "        if (rr2 < 0.6) and (1.8*rr2 < rr1) :\n",
    "            vf_episode = True\n",
    "            vf_quant = 1\n",
    "            classifcations[rr2_idx] = 3\n",
    "        \n",
    "        elif (\n",
    "            ((1.15*rr2 < rr1) and (1.15*rr2 < rr3)) \n",
    "            or ((abs(rr1 - rr2) < (0.3)) and (max(rr1,rr2) < (0.8)) and (rr3 > 1.2*((rr1 + rr2)/2)))\n",
    "            or ((abs(rr2 - rr3) < (0.3)) and (max(rr2,rr3) < (0.8)) and (rr1 > 1.2*((rr2 + rr3)/2))) \n",
    "              ):\n",
    "            classifcations[rr2_idx] = 2\n",
    "        elif(\n",
    "            ((2.2) < rr2 < (3.0))\n",
    "            and ((abs(rr1 - rr2) < (0.2)) or (abs(rr2 - rr3)) < (0.2))\n",
    "        ):\n",
    "            classifcations[rr2_idx] = 4\n",
    "            raise\n",
    "    else:\n",
    "        if (max(rr1, rr2, rr3) < (0.7)) or ((rr1 + rr2 + rr3) < (1.7)):\n",
    "            classifcations[rr2_idx] = 3  # Classify beat as VF\n",
    "            vf_quant += 1\n",
    "        else:\n",
    "            classifcations[rr2_idx] = 1  # Unecessary, but just to be clear\n",
    "            vf_episode = False\n",
    "\n",
    "            # If the episode was too short, classify all beats as normal\n",
    "            if vf_quant < 4:\n",
    "                for i in range(vf_quant):\n",
    "                    classifcations[rr2_idx - i] = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f021b72c1f7cd3898f2a25ac030750b22d22f61570d94b028ffadc231687c12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
